---
title: "Introduction to Regression"
subtitle: "March 6, 2023"
author: "Kara E. McCormack"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false
# figure options
# knitr::opts_chunk$set(
#   fig.width = 8, 
#   fig.asp = 0.618, 
#   out.width = "90%",
#   fig.retina = 3, 
#   dpi = 300, 
#   fig.align = "center"
# )
library(countdown)
```

# Welcome!

## Topics

::: nonincremental
-   Logistic regression + inference
-   Multivariate logistic regression + inference
-   Activity
:::

---

## Computational Setup

```{r}
#| echo: true
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

```{r}
#| echo: false
# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

---

## Assumptions

::: nonincremental
-   familiar with R and tidyverse
-   familiar with linear regression
-   interested in reviewing logistic regression + inference + visualization
:::

---

# Logistic regression


## Logistic regression

::: nonincremental
-   Logistic regression is a **generalized linear model** used to analyze two types of responses.
-   **Bernoulli**: Responses are either success $(Y=1)$ or failure $(Y=0)$

$$P(Y=y) = p^y(1-p)^{1-y}, \quad y=0, 1$$
-   **Binomial**: Response has $y$ successes among $n$ indep. Bernoulli trials w/ constant $P(\text{success})=p$

$$P(Y=y) = {n \choose y}p^y(1-p)^{n-y}, \quad y = 0, 1, \ldots, n$$
:::

---

## Bernoulli or Binomial?

Determine if the following are bernoulli or binomial. 

:::nonincremental
- Sample ruffed grouse (a type of bird) population in Michigan for presence of West Nile virus to estimate prevalence of virus in population.

- Use age, sex, and alcohol history to determine probability that an individual has liver disease.  
:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 1, 
          margin = "1.25%")
```

---

## Logit link

Binomial and bernoulli random variables can be written in one-parameter exponential family form, $f(y;\theta) = exp{[a(y)b(\theta) + c(\theta) + d(y)]}$

**Bernoulli**

$$f(y;\pi) = \exp\Big(y \log \Big(\frac{\pi}{1-\pi}\Big) + \log(1-\pi)\Big)$$

**Binomial**

$$f(y;n, \pi) = \exp\Big(y \log \Big(\frac{\pi}{1-\pi}\Big) + n\log(1-\pi) + \log {n \choose y} \Big)$$

We'll stick to bernoulli responses for now.

---

## Logistic regression, bernoulli response
::: nonincremental
-   Suppose response $Y$ takes value 1 with probability $\pi$ and value 0 with probability $1-\pi$. 

-   $\frac{\pi}{1-\pi}$: **odds** that $Y=1$

-   $\log\big(\frac{\pi}{1-\pi}\big)$: **log odds**

-   How do we get from $\pi$ to $\log\big(\frac{\pi}{1-\pi}\big)$? With the **logit transformation**.

:::


::: {.notes}
https://warpwire.duke.edu/w/pXgFAA/
:::

---

## Types of models

| Method                        | Response Type | Model |   
|-------------------------------|---------------|-------|
| Linear Regression             | Quantitative       | $Y = \beta_0 + \beta_1~ X$ |
| Logistic regression           | Binary        | $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0  + \beta_1 ~ X$ |  


---

## Odds to probabilities

**Odds**

$$\omega = \frac{\pi}{1-\pi}$$

**Probability**

$$\pi = \frac{\omega}{1+\omega}$$

---

## From odds to probabilities

::: nonincremental
-   **odds** $= \exp \{\log(\frac{\pi}{1-\pi})\}= \frac{\pi}{1-\pi}$
-   **logistic model**: log odds = $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 X$ 

- Combining these, we get:

$$\text{probability} = \pi = \frac{\exp\{\beta_0 + \beta_1 X\}}{1+ \exp\{\beta_0 + \beta_1 X\}}$$
:::



---

## Acupuncture example 

::: nonincremental
-   The `openintro::migraine` dataset is from a study about ear acupuncture in treatment of migraine attacks. 
-   **Response**: `pain_free` = yes or no
-   **Predictor**: `group` = control or treatment
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::


---

## Exploratory Data Analysis

::: question
::: nonincremental
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::
:::

```{r}
#| echo: false
#| fig-height: 4

migraine %>%
  ggplot(aes(x = group, fill = pain_free)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Acupuncture vs. Pain_free") +
  scale_fill_brewer(palette = "Set2", 
                    direction = -1) +
  coord_flip()

```

::: {.notes}
G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173-175. 

The majority of the points were located on the antero-
internal part of the antitragus (area M) on the same side of pain. The aim of this study was to verify the therapeutic value of area M and to compare it with an area of the ear (representation of the sciatic nerve, area S) which probably does not have a therapeutic effect on migraine attacks.
:::

---

## Modeling being pain-free

```{r}
#| echo: true

acu_model <- glm(pain_free ~ group, 
                  data  = migraine, 
                 family = "binomial")
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

::: poll
$$\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -3.091 + 1.897 \times \text{treatment}$$
:::

---

## Interpreting **treatment** coefficient - log odds

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```
The **log-odds** of being pain-free post-treatment are expected to be 1.897 higher for those who received treatment compared to those who did not receive treatment. 


---

## Interpreting **treatment** coefficient - odds

```{r}
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

The **odds** of being pain-free post-treatment for those who received treatment are expected to be 6.67 (i.e. exp(1.897)) times the odds for those who received the control.


---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

::: nonincremental
-   $H_0$: There is no linear relationship between the variable of interest and the log-odds of the response.

-   $H_A$: There **is** a linear relationship between the variable of interest and the log-odds of the response.
:::

---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

**Test statistic**:

$$z = \frac{\hat{\beta_j}-0}{SE_{\hat{\beta}_j}}$$

**P-value**: $P(|Z|>|z|)$, where $Z\sim N(0,1)$. 

---

## Confidence interval for $\beta_j$

Can calculate a **C% confidence interval** for $\beta_j$:

$$\hat{\beta_j} \pm z^* SE_{\hat{\beta_j}}$$

where $z^*$ comes from $N(0,1)$.


This is an interval for the change in log-odds of the response for a one-unit increase in $x_j$.

---

## Interpretation in terms of odds

The change in **odds** for every one-unit change in $x_j$. 

$$\exp{\hat{\beta}_j \pm z^* SE_{\hat{\beta}_j}}$$

**Interpretation**: We are $C$% confident that for every one-unit increase in $x_j$, the odds multiply by a factor of $\big\{\exp{\hat{\beta}_j - z^* SE_{\hat{\beta}_j}}\big\}$ to $\big\{\exp{\hat{\beta}_j + z^* SE_{\hat{\beta}_j}}\big\}$, holding all other variables constant. 



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Test statistic**

$$z = \frac{1.897-0}{0.808}= 2.34778$$

---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```


**P-value**

$$P(|Z| > |2.34778|)$$

```{r}
2 * pnorm(2.34778, lower.tail = FALSE)
```



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Conclusion**: Since the p-value is quite small, we reject $H_0$. The data provide sufficient evidence that the acupuncture treatment is a statistically significant predictor of being migraine-pain-free post-treatment.


# Multivariable Logistic Regression

## Multivariate response

::: nonincremental

-   Suppose our response variable $y$ takes on multiple categories $1, \ldots, K$ 


-   **Multinomial distribution**: 

$$P(y=1) = \pi_1, P(y=2) = \pi_2, \ldots, P(y=K) = \pi_K$$

with $\sum_{k=1}^K \pi_k = 1$


:::

---

## Multinomial logistic regression

::: nonincremental
-   Choose a baseline category for the response (i.e. $y=1$). 


$$\log\Big(\frac{\pi_{ik}}{\pi_{i1}}\Big)=\beta_{0k} + \beta_{1k}x_i$$

- There is a separate equation for each level of response, relative to baseline category.

:::

::: question
If we have $K$ categories of the response, how many equations will we have as part of our multinomial logistic regression model?
:::

---

## NHANES data

::: nonincremental
-   American National Health and Nutrition Examination Survey, NHANES R package, collected by the National Center for Health Statistics (NCHS)
-   Survey: Individuals of all ages complete a health exam.
-   Data from 2009-2010 and 2011-2012 sample years
-   R package data adapted for educational purposes, not suitable for research
-   For research purposes, download original files from [NCHS website](http://www.cdc.gov/nchs/nhanes.htm)
-   `?NHANES` in R for list of variables

:::

---

## Self-reported health vs. Age & Sleep Trouble

::: nonincremental
-   **Research question**: Is there an association between age, trouble sleeping, and self-reported health status?
-   Variables:

    - **HealthGen**: self-reported health rating: Poor, Fair, Good, VGood, or Excellent.
    
    - **Age**: age (years) at time of screening. Participants > 80 recorded as 80.
    
    - **SleepTrouble**: has told doctor that they had trouble sleeping: Yes or No. 
  
:::

---

## The data

```{r}
library(NHANES)
nhanes_adult <- NHANES %>%
  filter(Age >= 18) %>%
  select(HealthGen, Age, SleepTrouble) %>%
  drop_na() %>%
  mutate(obs_num = 1:n())
```

```{r}
#| echo: false
nhanes_adult %>%
  head() %>%
  kable()
```




## Exploratory Data Analysis

::: {.panel-tabset}
### Age

```{r}
#| echo: false
ggplot(data = nhanes_adult, aes(x = Age)) + 
  geom_histogram() +
  labs(title = "Distribution of Age")
```

### Trouble Sleeping

```{r}
#| echo: false
ggplot(data = nhanes_adult, aes(x = SleepTrouble)) + 
  geom_bar() +
  labs(title = "Has had trouble sleeping")
```

### Self-Reported Health

```{r}
#| echo: false
ggplot(data = nhanes_adult, aes(x = HealthGen)) + 
  geom_bar() +
  labs(title = "Self-reported rating of overall health")
```

:::


---

## Exploratory data analysis


::: {.panel-tabset}
### Age vs. Health rating
```{r}
#| echo: false
#| fig-height: 5
ggplot(data = nhanes_adult, aes(x = HealthGen, y = Age)) +
  geom_boxplot(fill = "#fc8d59") + 
  labs(title = "Age vs. Health Rating") +
  coord_flip()
```

### Sleep trouble vs. Health rating
```{r}
#| echo: false
#| fig-height: 3
ggplot(data = nhanes_adult, aes(x = SleepTrouble, 
                                fill = HealthGen)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Sleep Trouble vs. Health Rating") +
  scale_fill_brewer(palette = "Spectral", 
                    direction = -1)
```
:::

---

## Model in R

::: nonincremental
-   Use the `multinom()` function in the **nnet** R package. 

```{r results = 'hide'}
library(nnet)
health_m <- multinom(HealthGen ~ Age + SleepTrouble, 
                     data = nhanes_adult)
```

:::

---

## Output results
```{r}
#| echo: true
#| output-location: slide
tidy(health_m, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(digits = 3, format = "markdown")
```


---

## Poor vs. Excellent health

```{r}
#| echo: false
tidy(health_m, conf.int = TRUE, exponentiate = FALSE) %>%
  filter(y.level == "Poor") %>%
  kable(digits = 3, format = "markdown")
```

::: nonincremental
-   Baseline category of health rating is **Excellent**.
-   Model equation: the log odds that a person rates themselves "Poor" vs "Excellent" health is

$$\log\Big(\frac{\hat{\pi}_{Poor}}{\hat{\pi}_{Excellent}}\Big) = -3.567 + 0.031 \cdot \text{Age} + 1.669 \cdot \text{SleepTrouble}$$
:::

---

## Interpretations

::: incremental
$$\log\Big(\frac{\hat{\pi}_{Poor}}{\hat{\pi}_{Excellent}}\Big) = -3.567 + 0.031 \cdot \text{Age} + 1.669 \cdot \text{SleepTrouble}$$

For each additional year of age, the odds a person rates themselves as having poor health vs. excellent health are expected to multiply by 1.031 (exp(0.031)), assuming they have no sleep trouble. 

For those who have trouble sleeping, the odds they rate themselves as having poor health versus excellent health are expected to multiply by 5.306 (exp(1.669)), holding age constant. 
:::


---

## Interpretations: intercept
$$\log\Big(\frac{\hat{\pi}_{Poor}}{\hat{\pi}_{Excellent}}\Big) = -3.567 + 0.031 \cdot \text{Age} + 1.669 \cdot \text{SleepTrouble}$$
::: question
What is the interpretation for the intercept of this model, in terms of odds?
:::

---

## Interpretations: intercept
$$\log\Big(\frac{\hat{\pi}_{Poor}}{\hat{\pi}_{Excellent}}\Big) = -3.567 + 0.031 \cdot \text{Age} + 1.669 \cdot \text{SleepTrouble}$$

The odds a 0 year-old person without sleep trouble rates themselves as having poor health versus excellent health are 0.028 (exp(-3.567)).

-   Would need to mean-center age for the intercept to have a meaningful interpretation.

---

## Confidence Interval for Sleep Trouble

```{r}
#| echo: false
tidy(health_m, conf.int = TRUE, exponentiate = FALSE) %>%
  filter(y.level == "Poor") %>%
  kable(digits = 3, format = "markdown")
```

We are 95% confident that, if someone has trouble sleeping, the odds the person rates themselves as poor health vs excellent health will multiply by 3.735 (exp(3.138)) to 7.538 (exp(2.020)), holding age constant.

---

### Visualization

```{r}
#| echo: true
model_coef <- tidy(health_m, exponentiate = TRUE, conf.int = TRUE) %>%
  filter(y.level %in% c("Fair", "Poor"))
ggplot(data = model_coef, aes(x = term, y = estimate)) +
  geom_point() +
  geom_hline(yintercept = 1, lty = 2) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))+
  labs(title = "Exponentiated model coefficients") + 
  coord_flip() +
  facet_wrap(~y.level)
```


---

## Recap

::: nonincremental
-   Logistic regression + inference, acupuncture example
-   Multivariate logistic regression + inference, health rating example
-   Next up, activity
:::

# Activity

## Regression Bingo Game

::: nonincremental
- Pair up - two people per bingo card.
- Each square on bingo card has a question.
- "Answers" located throughout room. If you think you've found a correct answer, take a sticker and place it in the square. 
  -   Write a note on your card about what the answer said
- When you get bingo (3 in a row), shout it out and share your 3 question/answers. 
- If you'd like to see any slide from this lecture, feel free to ask!

:::


---

## Acknowledgements

::: nonincremental
-   [BMLR Chapter 6](https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html#introduction-to-logistic-regression)

:::


---



## End of class survey


![](./img/qr_code_google_form.png){.absolute top="170" left="30" width="500" height="500"}


# That's all, folks!
