---
title: "Logistic Regression"
subtitle: "March 6, 2023"
author: "Kara E. McCormack"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false
# figure options
# knitr::opts_chunk$set(
#   fig.width = 8, 
#   fig.asp = 0.618, 
#   out.width = "90%",
#   fig.retina = 3, 
#   dpi = 300, 
#   fig.align = "center"
# )
library(countdown)
```

## Topics

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

...with activities along the way!

---

## Assumptions



```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```


```{r}
#| echo: false
# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

I'm assuming you're familiar with:

::: nonincremental
-   Linear regression
-   Bernoulli and binomial distributions
-   R and tidyverse
:::

# Motivation

## Review: linear regression

```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

::: nonincremental
- Linear regression is used to describe the relationship between a quantitative predictor and a quantitative response variable. 
- Estimate the slope and intercept of the regression line using least squares method. 
- Interpret slope and intercept. 
:::

```{r}
#| echo: false
library(fivethirtyeight)

movie_scores <- fandango %>%
  rename(critics = rottentomatoes, 
         audience = rottentomatoes_user)
```

---

## Example: movie ratings data 

The data set contains the critics' score (**`critics`**)  and audience score (**`audience`**) for 146 movies rated on rottentomatoes.com.

```{r}
#| echo: false
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```

---

## A linear model for movie ratings

We fit a linear regression model to describe the relationship between the critics score and audience score.

```{r}
#| echo: false
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "coral2", se = FALSE) +
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```



---

## Dichotomous outcomes  {.smaller}

::: nonincremental
- Now, what if we had a 0/1 outcome instead?
- Suppose we observe whether or not a basketball player makes a free throw shot. 
- Let $Y=1$ if they make the shot, and $Y=0$ if they miss. 
- Can we fit a linear regression model?
:::

```{r}
#| echo: false
set.seed(56)
dat <- tibble(x=runif(100, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(100, 1, p),
                  y2=.3408+.0901*x,
                  logit=log(p/(1-p)))
dat2 <- tibble(x = c(dat$x, dat$x),
               y = c(dat$y2, dat$p),
               `Regression model` = c(rep("linear", 100),
                                      rep("logistic", 100)))
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  ylim(-.25, 1.25) +
  theme_bw()
```


---

## Poor fit of linear regression {.smaller}

::: nonincremental
- Linear regression is a poor fit to the data.
- Here, we'd like to model the probability of success, not the actual $Y$ values. 
- Linear model predicts values above 1 and below 0. 
- We can do better!
:::
```{r}
#| echo: false
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2 %>% filter(`Regression model` == "linear"), aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



---



## Linear vs. logistic regression {.smaller}

::: nonincremental
-   Logistic regression (dashed curve) follows data closely and always produces predicted probabilities between 0 and 1. 
:::


```{r}
#| echo: false
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2, aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



::: {.notes}
The solid line is a linear regression fit with least squares to the probability of success (Y=1) for a given value of X. With a binary response, the line doesn't fit the data well, and produces predicted probabilities below 0 and above 1. On the other hand, logistic regression (dashed curve) follows data closely and always produces predicted probabilities between 0 and 1. 
:::

---

### Bernoulli + binomial random variables {.smaller}

::: nonincremental
- **Bernoulli**: $Y$ can take one of two values, success ($Y=1$) or failure ($Y=0$) 
  - Let $P(Y=1)=\pi$, and $P(Y=0) = 1-\pi$
  - Then $Y \sim \text{Bernoulli}(\pi)$
    - $P(Y=y) = \pi^y (1-\pi)^{1-y}$ for $y=0, 1$
- **Binomial**: $Y$ is the number of successes in $n$ bernoulli trials, each with probability of success $\pi$ 
  - $Y \sim \text{Binomial},n, \pi)$ 
  - $P(Y=y) = {n \choose y} \pi^y(1-\pi)^{n-y}$ for $y=0, 1, \ldots, n$
:::

---

## Binomial or Bernoulli? {.smallish}

::: nonincremental

1. Is exposure to a particular chemical associated with a cancer diagnosis?
2. Absenteeism data are collected for 146 randomly selected students in New South Wales, Australia across one school year. Are demographic characteristics of children associated with absenteeism? 

To submit answers: $\quad \quad \quad \quad \quad$ or click [here](https://forms.gle/Y8fwCZz3svXuQV8G6). 
:::

![](./img/qr_binomial_bernoulli_quiz.png){.absolute bottom="20" left="300" width="200" height="300"}

```{r}
#| echo: false
library(countdown)
countdown(minutes = 1,
          seconds = 30,
          margin = "1.25%")
```


---

## Binomial or Bernoulli? Answers {.smallish}

::: incremental

1. Is exposure to a particular chemical associated with a cancer diagnosis?

- Bernoulli: The outcome is whether or not a person was diagnosed with cancer.

2. Absenteeism data are collected for 146 randomly selected students in New South Wales, Australia across one school year. Are demographic characteristics of children associated with absenteeism? 

- Binomial: The outcome is the number of days a student was absent out of $n$ days in a school year. 

:::

---

## Brainstorming predictors

In groups of 3-4, brainstorm some predictors for the following outcomes. 
::: nonincremental
1. $Y$ = whether or not teenagers get 7+ hours of sleep per night.
2. $Y$ = whether or not basketball players make their free-throw shots during a game.
3. $Y$ = whether or not acupuncture treatment provides pain-relief for individuals with migraines.
4. $Y$ = Whether a person self-rates their health status as "Good" or "Poor". 
:::

---


## Logistic regression setup
::: nonincremental
-   Suppose response $Y$ takes value 1 with probability $\pi$ and value 0 with probability $1-\pi$. Let $X$ be a predictor.

-   **odds** that $Y=1$ = $\frac{\pi}{1-\pi}$: 
-   Example: If $p(\text{win}) = .6$, then $\text{odds}(\text{win}) = \frac{.6}{1-.6} = 1.5$.

-   **log odds** = $\log\big(\frac{\pi}{1-\pi}\big)$

-   How do we get from $\pi$ to $\log\big(\frac{\pi}{1-\pi}\big)$? With the **logit transformation**.

:::


::: {.notes}
https://warpwire.duke.edu/w/pXgFAA/
Unc vs duke on saturday march 4
acc tournament march 7-11.
:::

---


## Odds to probabilities

::: nonincremental
- We've seen how to get from probability to odds, now how about odds to probability?
:::

**Odds**

$$\omega = \frac{\pi}{1-\pi}$$

**Probability**

$$\pi = \frac{\omega}{1+\omega}$$

---

## From odds to probabilities

::: nonincremental
-   **logistic model**: $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 X$ 
-   **odds** $= \exp \{\log(\frac{\pi}{1-\pi})\}= \frac{\pi}{1-\pi}$

- **probability model**: combining this w/ previous slide, we get:

$$\text{probability} = \pi = \frac{\exp\{\beta_0 + \beta_1 X\}}{1+ \exp\{\beta_0 + \beta_1 X\}}$$
:::

::: {.notes}
We can use our logistic regression model to calculate probability. 
:::

---

## Acupuncture example 

::: nonincremental
-   The `openintro::migraine` dataset is from a study about ear acupuncture in treatment of migraine attacks.
-   Treatment (area of ear associated with headache) vs. "placebo" treatment (area of ear associated with sciatica). 
-   **Response**: `pain_free` = yes or no
-   **Predictor**: `group` = control or treatment
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::


---

## Exploratory Data Analysis

::: question
::: nonincremental
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::
:::

```{r}
#| echo: false
#| fig-height: 4

migraine %>%
  ggplot(aes(x = group, fill = pain_free)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Acupuncture vs. Pain_free") +
  scale_fill_brewer(palette = "Set2", 
                    direction = -1) +
  coord_flip()

```

::: {.notes}
G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173-175. 

The majority of the points were located on the antero-
internal part of the antitragus (area M) on the same side of pain. The aim of this study was to verify the therapeutic value of area M and to compare it with an area of the ear (representation of the sciatic nerve, area S) which probably does not have a therapeutic effect on migraine attacks.
:::

---

## Modeling being pain-free

```{r}
#| echo: true

acu_model <- glm(pain_free ~ group, 
                  data  = migraine, 
                 family = "binomial")
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

::: poll
$$\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -3.091 + 1.897 \times \text{treatment}$$
:::



---

## Interpreting **treatment** coefficient - log odds

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```
The **log-odds** of being pain-free post-treatment are expected to be 1.897 higher for those who received treatment compared to those who did not receive treatment. 


---

## Interpreting **treatment** coefficient - odds

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

The **odds** of being pain-free post-treatment for those who received treatment are expected to be 6.67 (i.e. exp(1.897)) times the odds for those who received the control.

---


## Predicted log odds

```{r}
#| echo: false
#| eval: true

predict(acu_model) [1:7]
```

```{r}
#| echo: true
#| eval: false
predict(acu_model)
```
For person 1:

the predicted odds = 

$$\hat{\omega} = \frac{\hat{\pi}}{{1-\hat{\pi}}} = \exp(-1.1939) = 0.303$$

---

## Predicted probabilities

```{r}
#| echo: true
#| eval: false
predict(acu_model, 
        type = "response") #<<
```

```{r}
#| echo: false
predict(acu_model, 
        type = "response")[1:7]
```

For person 1:

predicted probability = 

$$\hat{\pi} = \frac{\exp{(-1.1939)}}{1+\exp{(-1.1939)}} = .232$$ 


---

## Logistic regression: a GLM {.small}

::: nonincremental
-   Logistic regression is a **generalized linear model** in which we can analyze data with a dichotomous response with  $P(\text{success}=\pi)$.
-   **Bernoulli**: Responses are either success $(Y=1)$ or failure $(Y=0)$

$$P(Y=y) = \pi^y(1-\pi)^{1-y}, \quad y=0, 1$$

-   **Binomial**: Each observation has $n$ bernoulli trials, each with $P(\text{success})=\pi$.

$$P(Y=y) = {n \choose y}\pi^y(1-\pi)^{(n-y)} $$
:::


---



---



## Exponential form {.smallish}

A bernoulli random variable can be written in one-parameter exponential family form, $f(y;\theta) = \exp{[a(y)b(\theta) + c(\theta) + d(y)]}$

**Bernoulli**

$$f(y;\pi) = \exp\Big[y \log \Big(\frac{\pi}{1-\pi}\Big) + \log(1-\pi)\Big]$$


::: question
What are $a(y), b(\pi), c(\pi)$, and $d(y)$?
:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 1, 
          margin = "1.25%")
```



---

## Exponential form {.smallish}

A bernoulli random variable can be written in one-parameter exponential family form, $f(y;\theta) = \exp{[a(y)b(\theta) + c(\theta) + d(y)]}$

**Bernoulli**

$$f(y;\pi) = \exp\Big[y \log \Big(\frac{\pi}{1-\pi}\Big) + \log(1-\pi)\Big]$$


::: question
What are $a(y), b(\pi), c(\pi)$, and $d(y)$?
:::

$a(y) = y$, $b(\pi)= \log \Big(\frac{\pi}{1-\pi}\Big)$, $c(\pi) = \log(1-\pi)$, and $d(y) = 0$.

$b(\pi)$ is the **canonical link function**. 


---


## Assumptions of logistic regression


::: nonincremental
1. **Binary responses**: Response is dichotomous (only takes on two values), or is the sum of dichotomous responses.
2. **Independence**: Observations independent of one another. 
3. **Variance structure**: Variance of binomial random variable is $n\pi(1-\pi)$, variance highest when $\pi=0.5$.
4. **Linearity**: Log of the odds ratio, $\log (\frac{\pi}{1-\pi})$, is a linear function of $x$.
:::





---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

::: nonincremental
-   $H_0$: There is no linear relationship between the variable of interest and the log-odds of the response.

-   $H_A$: There **is** a linear relationship between the variable of interest and the log-odds of the response.
:::

---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

**Test statistic**:

$$z = \frac{\hat{\beta_j}-0}{SE_{\hat{\beta}_j}}$$

**P-value**: $P(|Z|>|z|)$, where $Z\sim N(0,1)$. 

---

## Confidence interval for $\beta_j$

Can calculate a **C% confidence interval** for $\beta_j$:

$$\hat{\beta_j} \pm z^* SE_{\hat{\beta_j}}$$

where $z^*$ comes from $N(0,1)$.


This is an interval for the change in log-odds of the response for a one-unit increase in $x_j$.

---

## Interpretation in terms of odds

The change in **odds** for every one-unit change in $x_j$. 

$$\exp{\hat{\beta}_j \pm z^* SE_{\hat{\beta}_j}}$$

**Interpretation**: We are $C$% confident that for every one-unit increase in $x_j$, the odds multiply by a factor of $\big\{\exp{\hat{\beta}_j - z^* SE_{\hat{\beta}_j}}\big\}$ to $\big\{\exp{\hat{\beta}_j + z^* SE_{\hat{\beta}_j}}\big\}$, holding all other variables constant. 



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Test statistic**

$$z = \frac{1.897-0}{0.808} = 2.34778$$

---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```


**P-value**

$$P(|Z| > |2.34778|)$$

```{r}
2 * pnorm(2.34778, lower.tail = FALSE)
```



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Conclusion**: Since the p-value is quite small, we reject $H_0$. The data provide sufficient evidence that the acupuncture treatment is a statistically significant predictor of being migraine-pain-free post-treatment.


---

## Computational Setup

```{r}
#| echo: true
#| eval: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

---

## Acupuncture example with code


# Activity

## Regression Bingo Game

::: nonincremental
- Pair up - two people per bingo card.
- Each square on bingo card has a question.
- "Answers" located throughout room. If you think you've found a correct answer, tear off answer and place it on the square. 
  -   Write a note on your card about what the answer said
- When you get bingo (3 in a row), shout it out and share your 3 question/answers. 
- If you'd like to see any slide from this lecture, feel free to ask!

:::


---

## Recap

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

---

## Acknowledgements

::: nonincremental
-   [BMLR Chapter 6](https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html#introduction-to-logistic-regression)

-   [Introduction to Modern Statistics, Chapter 9](https://openintro-ims.netlify.app/model-logistic.html#model-logistic)

-   [STA210: Regression Analysis](https://sta210-fa21.netlify.app/)

:::


# That's all, folks!
