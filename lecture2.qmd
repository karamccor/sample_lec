---
title: "Logistic Regression"
subtitle: "March 6, 2023"
author: "Kara E. McCormack"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false
# figure options
# knitr::opts_chunk$set(
#   fig.width = 8, 
#   fig.asp = 0.618, 
#   out.width = "90%",
#   fig.retina = 3, 
#   dpi = 300, 
#   fig.align = "center"
# )
library(countdown)
```

## Topics

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

...with activities along the way!

---

## Assumptions



```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```


```{r}
#| echo: false
# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

I'm assuming you're familiar with:

::: nonincremental
-   Linear regression
-   Bernoulli and binomial distributions
-   R and tidyverse
:::

# Motivation

## Review: linear regression

```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

::: nonincremental
- Linear regression is used to describe the relationship between a quantitative predictor and a quantitative response variable. 
- Estimate the slope and intercept of the regression line using least squares method. 
- Interpret slope and intercept. 
:::

```{r}
#| echo: false
library(fivethirtyeight)

movie_scores <- fandango %>%
  rename(critics = rottentomatoes, 
         audience = rottentomatoes_user)
```

---

## Example: movie ratings data 

Dataset contains critics' score (**`critics`**)  and audience score (**`audience`**) for 146 movies rated on rottentomatoes.com.

```{r}
#| echo: false
#| fig-width: 10
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```

---

## A linear model for movie ratings

We fit a linear regression model to describe the relationship between the critics score and audience score.

```{r}
#| echo: false
#| fig-width: 10
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "coral2", se = FALSE) +
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```



---

## Binary outcomes  {.smaller}

::: nonincremental
- Now, what if we had a 0/1 outcome instead?
- Suppose we observe whether or not a basketball player makes a free throw shot. 
- Let $Y=1$ if they make the shot, and $Y=0$ if they miss. 
- Can we fit a linear regression model?
:::

```{r}
#| echo: false
#| fig-width: 8.5
set.seed(56)
dat <- tibble(x=runif(100, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(100, 1, p),
                  y2=.3408+.0901*x,
                  logit=log(p/(1-p)))
dat2 <- tibble(x = c(dat$x, dat$x),
               y = c(dat$y2, dat$p),
               `Regression model` = c(rep("linear", 100),
                                      rep("logistic", 100)))
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  ylim(-.25, 1.25) +
  theme_bw()
```


---

## Poor fit of linear regression {.smaller}

::: nonincremental
- Linear regression is a poor fit to the data.
- Here, we'd like to model the probability of success, not the actual $Y$ values. 
- Linear model predicts values above 1 and below 0. 
- We can do better!
:::
```{r}
#| echo: false

ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2 %>% filter(`Regression model` == "linear"), aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



---



## Linear vs. logistic regression {.smaller}

::: nonincremental
-   Logistic regression (dashed curve) follows data more closely.
-   Always produces predicted probabilities between 0 and 1. 
-   For this reason, we will focus on logistic regression for modeling binary or binomial responses. 
:::


```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 6
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2, aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



::: {.notes}
The solid line is a linear regression fit with least squares to the probability of success (Y=1) for a given value of X. With a binary response, the line doesn't fit the data well, and produces predicted probabilities below 0 and above 1. On the other hand, logistic regression (dashed curve) follows data closely and always produces predicted probabilities between 0 and 1. 
:::

---

### Bernoulli + binomial random variables {.smaller}

::: nonincremental
- **Bernoulli**: $Y$ can take one of two values, success ($Y=1$) or failure ($Y=0$) 
  - $P(Y=1)=\pi$, and $P(Y=0) = 1-\pi$
  - Then $Y \sim \text{Bernoulli}(\pi)$
  - $P(Y=y) = \pi^y (1-\pi)^{1-y}$ for $y=0, 1$
- **Binomial**: $Y$ is the number of successes in $n$ bernoulli trials, each with probability of success $\pi$ 
  - $Y \sim \text{Binomial}(n, \pi)$ 
  - $P(Y=y) = {n \choose y} \pi^y(1-\pi)^{n-y}$ for $y=0, 1, \ldots, n$
:::




---

## Binomial or Bernoulli? {.smaller}

For the following research questions, identify if the outcome of a single observation from the study is binomial or bernoulli. In pairs, submit answers on the survey via the QR code. 

::: nonincremental 
1. Is exposure to a particular air pollutant associated with probability that an individual is diagnosed with lung cancer?
2. At an elementary school, absenteeism data are collected for students across one school year. Are individual demographic characteristics associated with proportion of days a student is absent during a school year? 
3. A health survey asks individuals to self-rate their current health as "Poor", "Fair", "Good", or "Excellent". Do lifestyle factors affect probability of an individual self-rating their health as at least "Good"? (i.e. "Good" or "Excellent" vs. "Poor" or "Fair).
:::



![](./img/qr_binomial_bernoulli_quiz.png){.absolute top="150" right="-250" width="240" height="350"}

```{r}
#| echo: false
countdown(minutes = 2,
          seconds = 30,
          margin = "1.25%")
```

::: {.notes}
or click [here](https://forms.gle/Y8fwCZz3svXuQV8G6).
:::

---

## Binomial or Bernoulli? Answers {.smaller}

::: incremental

1. Is exposure to a particular air pollutant associated with probability that an individual is diagnosed with lung cancer?

- Bernoulli: The outcome is whether or not a person was diagnosed with lung cancer.

2. At an elementary school, absenteeism data are collected for students across one school year. Are individual demographic characteristics associated with percent of days a student is absent during one school year? 

- Binomial: The outcome is the number of days a student was absent out of $n$ days in a school year. 

3. A health survey asks individuals to self-rate their current health as "Poor", "Fair", "Good", or "Excellent". Do lifestyle factors affect probability of an individual self-rating their health as at least "Good"? (i.e. "Good" or "Excellent" vs. "Poor" or "Fair).

- Bernoulli: Each observation is whether or not an individual self-rates their health as "Good" or "Excellent" vs. "Poor" or "Fair on the survey.

:::

---

## Brainstorming predictors

In groups, brainstorm some predictors for the following outcomes. 

::: nonincremental
1. Whether or not teenagers get 7+ hours of sleep per night.
2. Whether or not basketball players make their free-throw shots during a game.
3. Whether or not acupuncture treatment provides pain-relief for individuals with migraines.
4. Whether a person self-rates their health status as "Good" or "Poor" on a health survey. 
:::

```{r}
#| echo: false
countdown(minutes = 2,
          margin = "1.25%")
```

---


## Setup: probabilities and odds
::: nonincremental
-   Suppose response $Y$ takes value 1 or 0 
-   Probability of success = $P(Y=1) = \pi$
-   Probability of failure = $P(Y=0) = 1-\pi$
-   Odds = $\frac{p(\text{success})}{p(\text{failure})} = \frac{\pi}{1-\pi}$
-   Example: If $p(\text{win}) = .6$, then $\text{odds}(\text{win}) = \frac{.6}{1-.6} = \frac{.6}{.4} = 1.5$.
-   In other words, the probability of winning is 1.5 times the probability of losing. 
:::

---

## Log odds

::: nonincremental
- Odds are difficult to model, since their range is from 0 to $\infty$. 
- So, we model log odds instead. 
- Log odds = $\log\big(\frac{\pi}{1-\pi}\big) = \text{logit}(\pi)$
- The logit function takes values between 0 and 1 (probabilities) and maps them to a range of - $\infty$ to + $\infty$. 





:::


::: {.notes}
https://warpwire.duke.edu/w/pXgFAA/
Unc vs duke on saturday march 4
acc tournament march 7-11.
:::

---

## Probability, odds, and log odds

| probability | odds   | log odds |
|-------------|:-------|:--------:|
| .01         | .01    |     -4.60|
| .05         | .05    |     -2.94|
| .10         |  .11   |     -2.20|
| .20         |  .25   |     -2.38|
| .30         |  .43   |     -0.84|
| .40         |  .67   |     -0.41|
| .50         |  1.00  |      0.00|
| .60         |  1.50  |      0.41|



---

## From log odds to probability

::: nonincremental
- In a logistic model, we are often interested in modeling $\pi$. 
- How can we get from log odds back to $\pi$?
::: 

---

## From log odds to probability {.smaller}

:::: {.columns}

::: {.column width="60%"}

::: incremental
$$
\begin{eqnarray*}
\omega &=& \log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \exp\log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \frac{\pi}{1-\pi}\\
\exp(\omega)(1-\pi) &=& \pi\\
\exp(\omega)-\pi \exp(\omega) &=& \pi\\
\exp(\omega)&=& \pi + \pi \exp(\omega) \\
\exp(\omega)&=& \pi(1 + \exp(\omega)) \\
\frac{\exp(\omega)}{1 + \exp(\omega)}&=& \pi 
\end{eqnarray*}
$$
:::
:::

::: {.column width="35%"}



:::

::::

---

## From log odds to probability {.smaller}

$$
\begin{eqnarray*}
\omega &=& \log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \exp\log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \frac{\pi}{1-\pi}\\
\exp(\omega)(1-\pi) &=& \pi\\
\exp(\omega)-\pi \exp(\omega) &=& \pi\\
\exp(\omega)&=& \pi + \pi \exp(\omega) \\
\exp(\omega)&=& \pi(1 + \exp(\omega)) \\
\frac{\exp(\omega)}{1 + \exp(\omega)}&=& \pi 
\end{eqnarray*}
$$



---

## From odds to probabilities

::: nonincremental
-   **logistic model**: $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 X$ 
-   **odds** $= \exp \{\log(\frac{\pi}{1-\pi})\}= \frac{\pi}{1-\pi}$

- **probability model**: combining this w/ previous slide, we get:

$$\text{probability} = \pi = \frac{\exp\{\beta_0 + \beta_1 X\}}{1+ \exp\{\beta_0 + \beta_1 X\}}$$
:::

::: {.notes}
We can use our logistic regression model to calculate probability. 
:::

---

## Acupuncture example 

::: nonincremental
-   The `openintro::migraine` dataset is from a study about ear acupuncture in treatment of migraine attacks.
-   Treatment (area of ear associated with headache) vs. "placebo" treatment (area of ear associated with sciatica). 
-   **Response**: `pain_free` = yes or no
-   **Predictor**: `group` = control or treatment
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::


---

## Exploratory Data Analysis

::: question
::: nonincremental
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::
:::

```{r}
#| echo: false
#| fig-height: 4

migraine %>%
  ggplot(aes(x = group, fill = pain_free)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Acupuncture vs. Pain_free") +
  scale_fill_brewer(palette = "Set2", 
                    direction = -1) +
  coord_flip()

```

::: {.notes}
G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173-175. 

The majority of the points were located on the antero-
internal part of the antitragus (area M) on the same side of pain. The aim of this study was to verify the therapeutic value of area M and to compare it with an area of the ear (representation of the sciatic nerve, area S) which probably does not have a therapeutic effect on migraine attacks.
:::

---

## Modeling being pain-free

```{r}
#| echo: true

acu_model <- glm(pain_free ~ group, 
                  data  = migraine, 
                 family = "binomial")
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

::: poll
$$\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -3.091 + 1.897 \times \text{treatment}$$
:::



---

## Interpreting **treatment** coefficient - log odds

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```
The **log-odds** of being pain-free post-treatment are expected to be 1.897 higher for those who received treatment compared to those who did not receive treatment. 


---

## Interpreting **treatment** coefficient - odds

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

The **odds** of being pain-free post-treatment for those who received treatment are expected to be 6.67 (i.e. exp(1.897)) times the odds for those who received the control.

---


## Predicted log odds

```{r}
#| echo: false
#| eval: true

predict(acu_model) [1:7]
```

```{r}
#| echo: true
#| eval: false
predict(acu_model)
```
For person 1:

the predicted odds = 

$$\hat{\omega} = \frac{\hat{\pi}}{{1-\hat{\pi}}} = \exp(-1.1939) = 0.303$$

---

## Predicted probabilities

```{r}
#| echo: true
#| eval: false
predict(acu_model, 
        type = "response") #<<
```

```{r}
#| echo: false
predict(acu_model, 
        type = "response")[1:7]
```

For person 1:

predicted probability = 

$$\hat{\pi} = \frac{\exp{(-1.1939)}}{1+\exp{(-1.1939)}} = .232$$ 


---


## Assumptions of logistic regression


::: nonincremental
1. **Binary responses**: Response is dichotomous (only takes on two values), or is the sum of dichotomous responses.
2. **Independence**: Observations independent of one another. 
3. **Variance structure**: Variance of binomial random variable is $n\pi(1-\pi)$, variance highest when $\pi=0.5$.
4. **Linearity**: Log of the odds ratio, $\log (\frac{\pi}{1-\pi})$, is a linear function of $x$.
:::





---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

::: nonincremental
-   $H_0$: There is no linear relationship between the variable of interest and the log-odds of the response.

-   $H_A$: There **is** a linear relationship between the variable of interest and the log-odds of the response.
:::

---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

**Test statistic**:

$$z = \frac{\hat{\beta_j}-0}{SE_{\hat{\beta}_j}}$$

**P-value**: $P(|Z|>|z|)$, where $Z\sim N(0,1)$. 

---

## Confidence interval for $\beta_j$

Can calculate a **C% confidence interval** for $\beta_j$:

$$\hat{\beta_j} \pm z^* SE_{\hat{\beta_j}}$$

where $z^*$ comes from $N(0,1)$.


This is an interval for the change in log-odds of the response for a one-unit increase in $x_j$.

---

## Interpretation in terms of odds

The change in **odds** for every one-unit change in $x_j$. 

$$\exp{\hat{\beta}_j \pm z^* SE_{\hat{\beta}_j}}$$

**Interpretation**: We are $C$% confident that for every one-unit increase in $x_j$, the odds multiply by a factor of $\big\{\exp{\hat{\beta}_j - z^* SE_{\hat{\beta}_j}}\big\}$ to $\big\{\exp{\hat{\beta}_j + z^* SE_{\hat{\beta}_j}}\big\}$, holding all other variables constant. 



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Test statistic**

$$z = \frac{1.897-0}{0.808} = 2.34778$$

---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```


**P-value**

$$P(|Z| > |2.34778|)$$

```{r}
2 * pnorm(2.34778, lower.tail = FALSE)
```



---

## Let's look at the coefficient for treatment

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Conclusion**: Since the p-value is quite small, we reject $H_0$. The data provide sufficient evidence that the acupuncture treatment is a statistically significant predictor of being migraine-pain-free post-treatment.


---

## Computational Setup

```{r}
#| echo: true
#| eval: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

---

## Acupuncture example with code


# Activity

## Regression Bingo Game

::: nonincremental
- Pair up - two people per bingo card.
- Each square on bingo card has a question.
- "Answers" located throughout room. If you think you've found a correct answer, tear off answer and place it on the square. 
  -   Write a note on your card about what the answer said
- When you get bingo (3 in a row), shout it out and share your 3 question/answers. 
- If you'd like to see any slide from this lecture, feel free to ask!

:::


---

## Recap

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

---

## Acknowledgements

::: nonincremental
-   [BMLR Chapter 6](https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html#introduction-to-logistic-regression)

-   [Introduction to Modern Statistics, Chapter 9](https://openintro-ims.netlify.app/model-logistic.html#model-logistic)

-   [STA210: Regression Analysis](https://sta210-fa21.netlify.app/)

:::


# That's all, folks!
