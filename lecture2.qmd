---
title: "Logistic Regression"
subtitle: "March 6, 2023"
author: "Kara E. McCormack"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
editor: visual
execute:
  freeze: auto
  echo: true
---

```{r}
#| include: false
# figure options
# knitr::opts_chunk$set(
#   fig.width = 8, 
#   fig.asp = 0.618, 
#   out.width = "90%",
#   fig.retina = 3, 
#   dpi = 300, 
#   fig.align = "center"
# )
library(countdown)
```

## Topics

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

...with activities along the way!

---

## Assumptions



```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```


```{r}
#| echo: false
# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

I'm assuming you're familiar with:

::: nonincremental
-   Linear regression
-   Multiple linear regression
-   Bernoulli and binomial distributions
-   R and tidyverse
:::

# Motivation

## Review: linear regression

```{r}
#| echo: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```

::: nonincremental
- Linear regression is used to describe the relationship between a quantitative predictor and a quantitative response variable. 
- Estimate the slope(s) and intercept of the regression function, including conditional interpretation when using more than one predictor.
- Obtain parameter estimates using either the principle of least squares or maximum likelihood 
- Interpret slope and intercept. 
:::

```{r}
#| echo: false
library(fivethirtyeight)

movie_scores <- fandango %>%
  rename(critics = rottentomatoes, 
         audience = rottentomatoes_user)
```

---

## Example: movie ratings data 

Dataset contains critics' score (**`critics`**)  and audience score (**`audience`**) for 146 movies rated on rottentomatoes.com.

```{r}
#| echo: false
#| fig-width: 10
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```

---

## A linear model for movie ratings

We fit a linear regression model to describe how well critics' scores correspond with audience scores.

```{r}
#| echo: false
#| fig-width: 10
ggplot(data = movie_scores, mapping = aes(x = critics, y = audience)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", color = "coral2", se = FALSE) +
  labs(x = "Critics Score" , 
       y = "Audience Score") +
  theme_bw()
```



---

## Binary outcomes  {.smaller}

::: nonincremental
- Now, what if we had a binary (0/1 = no/yes = failure/success) outcome instead?
- Suppose we want to estimate probability of making a free throw shot. We observe 100 free throw attempts, and assume independence. 
  - Let $Y=1$ if they make the shot, and $Y=0$ if they miss.
  - Let X be some covariate of interest.
  - Can we fit a linear regression model?
:::

```{r}
#| echo: false
#| fig-width: 8.5
set.seed(56)
dat <- tibble(x=runif(100, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(100, 1, p),
                  y2=.3408+.0901*x,
                  logit=log(p/(1-p)))
dat2 <- tibble(x = c(dat$x, dat$x),
               y = c(dat$y2, dat$p),
               `Regression model` = c(rep("linear", 100),
                                      rep("logistic", 100)))
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  ylim(-.25, 1.25) +
  theme_bw()
```


---

## Poor fit of linear regression {.smaller}

::: nonincremental
- Linear regression is a poor fit to the data.
- Here, we'd like to estimate the probability of success, not the actual $Y$ values. 
- Linear model can predict values above 1 and below 0. 
- We can do better!
:::
```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 8
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2 %>% filter(`Regression model` == "linear"), aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



---



## Linear vs. logistic regression {.smaller}

::: nonincremental
-   Logistic regression (dashed curve) follows data more closely.
-   Always produces predicted probabilities between 0 and 1. 
-   For this reason (and others), we will focus on logistic regression for modeling binary or binomial responses. 
:::


```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 8
ggplot() + 
  geom_point(data = dat, aes(x, y), alpha = 0.5) +
  geom_line(data = dat2, aes(x, y, linetype = `Regression model`)) +
  ylim(-.25, 1.25) +
  theme_bw()
```



::: {.notes}
The solid line is a linear regression fit with least squares to the probability of success (Y=1) for a given value of X. With a binary response, the line doesn't fit the data well, and produces predicted probabilities below 0 and above 1. On the other hand, logistic regression (dashed curve) follows data closely and always produces predicted probabilities between 0 and 1. 
:::

---

### Bernoulli and binomial random variables {.smaller}

::: nonincremental
- **Bernoulli**: $Y$ can take one of two values, success ($Y=1$) or failure ($Y=0$) 
  - $P(Y=1)=\pi$, and $P(Y=0) = 1-\pi$
  - Then $Y \sim \text{Bernoulli}(\pi)$
  - $P(Y=y) = \pi^y (1-\pi)^{1-y}$ for $y=0, 1$
- **Binomial**: $Y$ is the number of successes in $n$ bernoulli trials, each with probability of success $\pi$ 
  - $Y \sim \text{Binomial}(n, \pi)$ 
  - $P(Y=y) = {n \choose y} \pi^y(1-\pi)^{n-y}$ for $y=0, 1, \ldots, n$
:::




---

## Binomial or Bernoulli? {.smaller}

For the following research questions, identify if the outcome of a single observation from the study is binomial or bernoulli. In pairs, submit answers on the survey via the QR code. 

::: nonincremental 
1. Is exposure to a particular pollutant over a 5-year period a risk factor for an individual's diagnosis of lung cancer?
2. At an elementary school, absenteeism data are collected for students across 100 randomly selected days in a school year. Do percent of days absent differ among students with different demographic characteristics?
3. A health survey asks individuals to self-rate their current health as "Poor", "Fair", "Good", or "Excellent". Do lifestyle factors affect probability of an individual self-rating their health as at least "Good"? (i.e. "Good" or "Excellent" vs. "Poor" or "Fair).
:::



![](./img/qr_binomial_bernoulli_quiz.png){.absolute top="150" right="-250" width="240" height="350"}

```{r}
#| echo: false
countdown(minutes = 2,
          seconds = 30,
          margin = "1.25%")
```

::: {.notes}
or click [here](https://forms.gle/Y8fwCZz3svXuQV8G6).
:::

---

## Binomial or Bernoulli? Answers {.smaller}

::: incremental

1. Is exposure to a particular air pollutant associated with probability that an individual is diagnosed with lung cancer?

- Bernoulli: The outcome is whether or not a person was diagnosed with lung cancer.

2. At an elementary school, absenteeism data are collected for students across one school year. Are individual demographic characteristics associated with percent of days a student is absent during one school year? 

- Binomial: The outcome is the number of days a student was absent out of $n$ days in a school year. 

3. A survey asks individuals to self-rate their current health as "Poor", "Fair", "Good", or "Excellent". Do lifestyle factors affect probability of an individual self-rating their health as at least "Good"?

- Bernoulli: Each observation is whether or not an individual self-rates their health as {"Good" or "Excellent"} vs. {"Poor" or "Fair"} on the survey question.

:::

---

## Brainstorming predictors {.smallish}

In groups, brainstorm some predictors for the probability of the following outcomes. 

::: nonincremental
1. Whether a randomly selected teenager get 7+ hours of sleep per night in a 1-week period.
2. Whether or not a basketball player makes a free-throw shots during a game.
3. Whether or not a targeted acupuncture treatment provides pain-relief for individuals with migraines, as compared to a placebo treatment.
4. Whether a person self-rates their health status as at least "Good" on a single health survey question at a routine doctor visit. 
:::

```{r}
#| echo: false
countdown(minutes = 2,
          margin = "1.25%")
```


---


## Setup: probabilities and odds
::: nonincremental
-   Suppose response $Y$ takes value 1 or 0 
-   Probability of success = $P(Y=1) = \pi$
-   Probability of failure = $P(Y=0) = 1-\pi$
-   Odds = $\frac{p(\text{success})}{p(\text{failure})} = \frac{\pi}{1-\pi}$
-   Example: If $p(\text{win}) = .6$, then $\text{odds}(\text{win}) = \frac{.6}{1-.6} = \frac{.6}{.4} = 1.5$.
-   In other words, the probability of winning is 1.5 times the probability of losing. 
:::

---

## Log odds

::: nonincremental
- Odds are difficult to model, since their range is from 0 to $\infty$. 
- So, we model log odds instead. 
- Log odds = $\log\big(\frac{\pi}{1-\pi}\big) = \text{logit}(\pi)$
- The logit function takes values between 0 and 1 (probabilities) and maps them to a range of -$\infty$ to +$\infty$. 





:::


::: {.notes}
https://warpwire.duke.edu/w/pXgFAA/
Unc vs duke on saturday march 4
acc tournament march 7-11.
:::

---

## Probability, odds, and log odds {.smallish}

:::: {.columns}

::: {.column width="60%"}
| probability | odds   | log odds |
|-------------|:-------|:--------:|
| .01         | .01    |     -4.60|
| .05         | .05    |     -2.94|
| .10         |  .11   |     -2.20|
| .20         |  .25   |     -2.38|
| .30         |  .43   |     -0.84|
| .40         |  .67   |     -0.41|
| .50         |  1.00  |      0.00|
| .60         |  1.50  |      0.41|
:::

::: {.column width="40%"}

::: nonincremental
- When probabilities are small, they resemble the odds.
- Odds range from 0 to infinitely large
- Log odds range from $-\infty$ to $+\infty$.
:::

:::
::::


---

## From log odds to probability

::: nonincremental
- In a logistic model, we estimate log odds. 
- However, once we fit the model, we are often interested in estimating $\pi$.
- How can we get from log odds back to $\pi$?
::: 

---

## From log odds to probability {.smaller}

:::: {.columns}

::: {.column width="60%"}

::: incremental
$$
\begin{eqnarray*}
\omega &=& \log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \exp\log\Big(\frac{\pi}{1-\pi}\Big) \\
\exp(\omega) &=& \frac{\pi}{1-\pi}\\
\exp(\omega)(1-\pi) &=& \pi\\
\exp(\omega)-\pi \exp(\omega) &=& \pi\\
\exp(\omega)&=& \pi + \pi \exp(\omega) \\
\exp(\omega)&=& \pi(1 + \exp(\omega)) \\
\frac{\exp(\omega)}{1 + \exp(\omega)}&=& \pi 
\end{eqnarray*}
$$
:::
:::

::: {.column width="35%"}



:::

::::

---


## Logistic model {.smaller}

::: nonincremental

::: poll
$$\text{logit}(\pi) = \log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 X$$ 
:::

- **Assumptions**
  1. **Binary responses**: $Y_i$ is a binary variable where $Y_i \sim \text{Bernoulli}(\pi_i)$ for $i=1, \ldots, n$.
  2. **Independence**: $Y_i$ is independent from $Y_j$ for all $i \neq j$.
  3. **Linearity**: Log of the odds ratio, $\log (\frac{\pi}{1-\pi})$, is a linear in $\beta_0$, $\beta_1$.
- Combining with previous slide, we get: 

$$\pi = \frac{\exp\{\beta_0 + \beta_1 X\}}{1+ \exp\{\beta_0 + \beta_1 X\}}$$
:::

::: {.notes}
We can use our logistic regression model to calculate probability. 
:::

---

## Acupuncture example {.smaller}

:::: {.columns}

::: {.column width="60%"}
::: nonincremental
-   The `openintro::migraine` dataset is from a study about ear acupuncture in treatment of migraine attacks.
-   89 individuals received either acupuncture treatment vs. "placebo" treatment (sham acupuncture on area of ear assoc. w/ sciatica). 
-   **Response**: `pain_free` = yes or no
-   **Predictor**: `group` = control or treatment
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::
:::

::: {.column width="40%"}

:::
::::

![](./img/acu_ear.png){.absolute top="80" right="0" width="400" height="350"}

::: aside
G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173-175.
:::

---

## Exploratory Data Analysis

::: question
::: nonincremental
-   **Research question**: Is acupuncture treatment associated with a reduction of pain?
:::
:::

```{r}
#| echo: false
#| fig-height: 4

migraine %>%
  ggplot(aes(x = group, fill = pain_free)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "Acupuncture vs. Pain_free") +
  scale_fill_brewer(palette = "Set2", 
                    direction = -1) +
  coord_flip()

```

::: {.notes}
G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173-175. 

The majority of the points were located on the antero-
internal part of the antitragus (area M) on the same side of pain. The aim of this study was to verify the therapeutic value of area M and to compare it with an area of the ear (representation of the sciatic nerve, area S) which probably does not have a therapeutic effect on migraine attacks.
:::

---

## Setting up the model

::: nonincremental
- Want to fit logistic regression model to acupuncture data.
- Let $Y_i$ be the response for individual $i$, i.e. whether the person reported being pain-free post-treatment.
- Let $P(Y_i = 1) = \pi_i$.
- Let $I(\cdot)$ be an indicator function.
$$\text{logit}(\pi_i) = \log\Big(\frac{\pi_i}{1-\pi_i}\Big) = \beta_0 + 
\beta_1\cdot I_i(\text{treatment})$$

:::

---

## Computational Setup

```{r}
#| echo: true
#| eval: false
# load packages
library(tidyverse)
library(tidymodels)
library(openintro)
library(knitr)
library(RColorBrewer)
```



---

## Fit the model

```{r}
#| echo: true
acu_model <- glm(pain_free ~ group, 
                  data  = migraine, 
                 family = "binomial")
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

::: poll
$$\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = -3.091 + 1.897 \cdot I(\text{treatment})$$
:::



---

## Log-odds by treatment group 


::: poll
$$\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = -3.091 + 1.897 \cdot I(\text{treatment})$$
:::

::: nonincremental
- For someone in the control group, $\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = -3.091$.
- For someone in the treatment group, $\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = -3.091 + 1.897 = -1.194$.
- $\beta_1=1.897$ measures the difference in log-odds for someone in the treatment group vs. the control group. 
:::


---

## Interpretations by treatment group 


::: poll
$$\log\Big(\frac{\hat{\pi}_i}{1-\hat{\pi}_i}\Big) = -3.091 + 1.897 \cdot I(\text{treatment})$$
:::

::: nonincremental
- For those in the treatment group, the **log-odds** of being pain-free post-treatment are expected to be 1.897 higher compared to those in the control group. 
- For those in the treatment group, the **odds** of being pain-free post-treatment are expected to be 6.67 (i.e. exp(1.897)) times the odds of those in the control group.
:::


---


## Predicted log odds and odds {.smallish}

```{r}
predict(acu_model)[1:7]
```



::: nonincremental
- We can use the `predict` function to get the predicted log odds for individuals. 
- Person 1 is in the treatment group.
- For person 1, predicted **log odds** = -1.1939.
- Then, we can exponentiate this value to get the predicted odds.
- For person 1, predicted **odds** = exp(-1.1939) = 0.303.
:::


---

## Predicted probabilities, treatment

```{r}
predict(acu_model, 
        type = "response")[1:7]
```

::: nonincremental
- We can use the `type = "response"` option to print the predicted probabilities instead. 
- For person 1 (treatment group), predicted probability = 
:::

$$\hat{\pi}_i = \frac{\exp{(-1.1939)}}{1+\exp{(-1.1939)}} = .232$$


---

## Predicted probabilities, control {.smallish}

```{r}
predict(acu_model)[83:89]
predict(acu_model, 
        type = "response")[83:89]
```

::: nonincremental
- We can compare this to the predicted probability for an individual in the control group.
- For person 83 (control group), predicted probability =  
:::

$$\hat{\pi}_i = \frac{\exp{(-3.091)}}{1+\exp{(-3.091)}} = .043$$
---

## Hypothesis test for $\beta_j$


::: nonincremental
- We've seen that $\beta_1$ estimates the difference in log odds between those in the treatment group versus those in the control. 
- Is $\beta_1$ large enough to determine statistical significance?
- **Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$
-   $H_0$: There is no linear relationship between the variable of interest and the log-odds of the response.
-   $H_A$: There **is** a linear relationship between the variable of interest and the log-odds of the response.
:::

---

## Hypothesis test for $\beta_j$

**Hypotheses**: $H_0: \beta_j = 0$ vs $H_A: \beta_j \neq 0$

**Test statistic**:

$$z = \frac{\hat{\beta_j}-0}{SE_{\hat{\beta}_j}}$$

**P-value**: $P(|Z|>|z|)$, where $Z\sim N(0,1)$. 

---

## Confidence interval for $\beta_j$

Can calculate a **C% confidence interval** for $\beta_j$:

$$\hat{\beta_j} \pm z^* SE_{\hat{\beta_j}}$$

where $z^*$ comes from $N(0,1)$.


This is an interval for the change in log-odds of the response for a one-unit increase in $x_j$.

---

## CI interpretation in terms of odds

The change in **odds** for every one-unit change in $x_j$. 

$$\exp\Big(\hat{\beta}_j \pm z^* SE_{\hat{\beta}_j}\Big)$$

**Interpretation**: We are $C$% confident that for every one-unit increase in $x_j$, the odds multiply by a factor of $\exp\Big({\hat{\beta}_j - z^* SE_{\hat{\beta}_j}}\big)$ to $\exp\Big({\hat{\beta}_j + z^* SE_{\hat{\beta}_j}}\big)$, holding all other variables constant. 



---

## Test statistic

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Test statistic**

$$z = \frac{1.897-0}{0.808} = 2.34778$$

---

## P-value

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```


**P-value**

$$P(|Z| > |2.34778|)$$

```{r}
2 * pnorm(2.34778, lower.tail = FALSE)
```



---

## Conclusion

```{r}
#| echo: false
acu_model %>%
  tidy %>%
  kable(digits = 3)
```

**Conclusion**: Since the p-value is quite small, we reject $H_0$. The data provide sufficient evidence that the acupuncture treatment is a statistically significant predictor of being migraine-pain-free post-treatment.


# Activity

## Regression Bingo Game

::: nonincremental
- Form groups of 2-3 people per bingo card.
- Each square on bingo card has a question.
- "Answers" located throughout room. If you think you've found a correct answer, tear off answer and place it on the square. 
- When you get bingo (3 in a row), shout it out and share your 3 question/answers. 
- If you'd like to see any slide from this lecture, feel free to ask!

:::


---

## Recap

::: nonincremental
-   Motivation
-   Odds and probabilities
-   Logistic regression model
-   Example
:::

---

## Acknowledgements

::: nonincremental
-   [BMLR Chapter 6](https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html#introduction-to-logistic-regression)

-   [Introduction to Modern Statistics, Chapter 9](https://openintro-ims.netlify.app/model-logistic.html#model-logistic)

-   [STA210: Regression Analysis](https://sta210-fa21.netlify.app/)

:::


# That's all, folks!
